{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "019541aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e17e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gujarati-aware regex patterns\n",
    "EMAIL_PATTERN = r'\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b'\n",
    "URL_PATTERN = r'https?://\\S+\\.[a-zA-Z]+|www\\.\\S+\\.[a-zA-Z]+'\n",
    "DECIMAL_PATTERN = r'\\d+\\.\\d+'\n",
    "DATE = r'(?:[0-9\\u0AE6-\\u0AEF]{1,2}[-/]){2}[0-9\\u0AE6-\\u0AEF]{2,4}|(?:[0-9\\u0AE6-\\u0AEF]{4}[-/])([0-9\\u0AE6-\\u0AEF]{1,2})[-/][0-9\\u0AE6-\\u0AEF]{1,2}'\n",
    "GUJARATI_LETTER = r'[\\u0A80-\\u0AFF]'\n",
    "WORD_PATTERN = r'[\\u0A80-\\u0AFF]+|[a-zA-Z0-9]+|[.,!?;:\"\\'‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî‚Äì-]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ccba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # File names\n",
    "# # input_file = \"gu_meta_part_1.txt\"\n",
    "# # sentence_file = \"gu_meta_part_1.txt_sentences.txt\"\n",
    "# # word_file = \"gu_meta_part_1.txt_words.txt\"\n",
    "# # recombined_file = \"gu_meta_part_1.txt_recombined.txt\"\n",
    "# # email_file = \"gu_meta_part_1.txt_emails.txt\"\n",
    "# # url_file = \"gu_meta_part_1.txt_url.txt\"\n",
    "input_file=\"\"\n",
    "sentence_file=\"\"\n",
    "word_file=\"\"\n",
    "recombined_file=\"\"\n",
    "email_file=\"\"\n",
    "url_file=\"\"\n",
    "# # Batch size (smaller = faster regex)\n",
    "# BATCH_SIZE = 10000\n",
    "\n",
    "# # Gujarati-aware regex patterns (compiled for speed)\n",
    "# EMAIL_PATTERN = re.compile(r'\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b')\n",
    "# URL_PATTERN = re.compile(r'https?://\\S+\\.[a-zA-Z]+|www\\.\\S+\\.[a-zA-Z]+')\n",
    "# DECIMAL_PATTERN = re.compile(r'\\d+\\.\\d+')\n",
    "# DATE_PATTERN = re.compile(\n",
    "#     r'(?:[0-9\\u0AE6-\\u0AEF]{1,2}[-/]){2}[0-9\\u0AE6-\\u0AEF]{2,4}'\n",
    "#     r'|(?:[0-9\\u0AE6-\\u0AEF]{4}[-/])([0-9\\u0AE6-\\u0AEF]{1,2})[-/][0-9\\u0AE6-\\u0AEF]{1,2}'\n",
    "# )\n",
    "# GUJARATI_LETTER = re.compile(r'[\\u0A80-\\u0AFF]')\n",
    "# WORD_PATTERN = re.compile(r'[\\u0A80-\\u0AFF]+|[a-zA-Z0-9]+|[.,!?;:\"\\'‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî‚Äì-]')\n",
    "\n",
    "# def process():\n",
    "#     with open(input_file, 'r', encoding='utf-8') as fin, \\\n",
    "#          open(sentence_file, 'w', encoding='utf-8') as f_sent, \\\n",
    "#          open(word_file, 'w', encoding='utf-8') as f_word, \\\n",
    "#          open(recombined_file, 'w', encoding='utf-8') as f_recombined, \\\n",
    "#          open(email_file, 'w', encoding='utf-8') as f_email, \\\n",
    "#          open(url_file, 'w', encoding='utf-8') as f_url:\n",
    "\n",
    "#         batch = []\n",
    "#         batch_num = 1\n",
    "\n",
    "#         for line_num, line in enumerate(fin, 1):\n",
    "#             batch.append(line)\n",
    "\n",
    "#             if line_num % BATCH_SIZE == 0:\n",
    "#                 process_batch(batch, batch_num, f_sent, f_word, f_recombined, f_email, f_url)\n",
    "#                 batch.clear()\n",
    "#                 batch_num += 1\n",
    "\n",
    "#         if batch:\n",
    "#             process_batch(batch, batch_num, f_sent, f_word, f_recombined, f_email, f_url)\n",
    "\n",
    "# def process_batch(batch_lines, batch_num, f_sent, f_word, f_recombined, f_email, f_url):\n",
    "#     # Email & URL extraction line-by-line\n",
    "#     for line in batch_lines:\n",
    "#         for e in EMAIL_PATTERN.findall(line):\n",
    "#             f_email.write(e + \"\\n\")\n",
    "#         for u in URL_PATTERN.findall(line):\n",
    "#             f_url.write(u + \"\\n\")\n",
    "\n",
    "#     # Sentence splitting\n",
    "#     batch_text = \" \".join(batch_lines)\n",
    "#     sentences = re.split(r'(?<=[.!?‡•§])\\s+', batch_text)\n",
    "\n",
    "#     all_tokens = []\n",
    "#     for sent in sentences:\n",
    "#         if not sent.strip():\n",
    "#             continue\n",
    "#         tokens = WORD_PATTERN.findall(sent)\n",
    "#         f_sent.write(\" \".join(tokens) + \"\\n\")\n",
    "#         f_recombined.write(\" \".join(tokens) + \"\\n\")\n",
    "#         all_tokens.extend(tokens)\n",
    "\n",
    "#     # Write words\n",
    "#     for word in all_tokens:\n",
    "#         f_word.write(word + \"\\n\")\n",
    "\n",
    "#     print(f\"‚úÖ Processed Batch {batch_num} - {len(batch_lines)} lines\")\n",
    "\n",
    "# # Run processing\n",
    "# # process()\n",
    "\n",
    "\n",
    "# import re\n",
    "\n",
    "# Files\n",
    "# input_file = \"gu_meta_part_3.txt\"\n",
    "# sentence_file = \"gu_meta_part_3_sentences.txt\"\n",
    "# word_file = \"gu_meta_part_3_words.txt\"\n",
    "# recombined_file = \"gu_meta_part_3_recombined.txt\"\n",
    "# email_file = \"gu_meta_part_3_emails.txt\"\n",
    "# url_file = \"gu_meta_part_3_url.txt\"\n",
    "\n",
    "BATCH_SIZE = 10000  # smaller batches for regex speed\n",
    "\n",
    "# Gujarati-aware regex patterns (compiled once)\n",
    "EMAIL_PATTERN = re.compile(r'\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b')\n",
    "URL_PATTERN = re.compile(r'https?://\\S+\\.[a-zA-Z]+|www\\.\\S+\\.[a-zA-Z]+')\n",
    "WORD_PATTERN = re.compile(r'[\\u0A80-\\u0AFF]+|[a-zA-Z0-9]+|[.,!?;:\"\\'‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî‚Äì-]')\n",
    "\n",
    "def process():\n",
    "    with open(input_file, 'r', encoding='utf-8') as fin, \\\n",
    "         open(sentence_file, 'w', encoding='utf-8') as f_sent, \\\n",
    "         open(word_file, 'w', encoding='utf-8') as f_word, \\\n",
    "         open(recombined_file, 'w', encoding='utf-8') as f_recombined, \\\n",
    "         open(email_file, 'w', encoding='utf-8') as f_email, \\\n",
    "         open(url_file, 'w', encoding='utf-8') as f_url:\n",
    "\n",
    "        batch = []\n",
    "        batch_num = 1\n",
    "\n",
    "        for line_num, line in enumerate(fin, 1):\n",
    "            batch.append(line)\n",
    "            if line_num % BATCH_SIZE == 0:\n",
    "                process_batch(batch, batch_num, f_sent, f_word, f_recombined, f_email, f_url)\n",
    "                batch.clear()\n",
    "                batch_num += 1\n",
    "\n",
    "        if batch:\n",
    "            process_batch(batch, batch_num, f_sent, f_word, f_recombined, f_email, f_url)\n",
    "\n",
    "def process_batch(batch_lines, batch_num, f_sent, f_word, f_recombined, f_email, f_url):\n",
    "    email_buf = []\n",
    "    url_buf = []\n",
    "    sentence_buf = []\n",
    "    word_buf = []\n",
    "    recombined_buf = []\n",
    "\n",
    "    # Email & URL line-by-line\n",
    "    for line in batch_lines:\n",
    "        email_buf.extend(EMAIL_PATTERN.findall(line))\n",
    "        url_buf.extend(URL_PATTERN.findall(line))\n",
    "\n",
    "    # Sentence splitting\n",
    "    batch_text = \" \".join(batch_lines)\n",
    "    sentences = re.split(r'(?<=[.!?‡•§])\\s+', batch_text)\n",
    "\n",
    "    for sent in sentences:\n",
    "        if not sent.strip():\n",
    "            continue\n",
    "        tokens = WORD_PATTERN.findall(sent)\n",
    "        sentence_buf.append(\" \".join(tokens) + \"\\n\")\n",
    "        recombined_buf.append(\" \".join(tokens) + \"\\n\")\n",
    "        word_buf.extend(word + \"\\n\" for word in tokens)\n",
    "\n",
    "    # Write buffered results\n",
    "    if email_buf:\n",
    "        f_email.write(\"\\n\".join(email_buf) + \"\\n\")\n",
    "    if url_buf:\n",
    "        f_url.write(\"\\n\".join(url_buf) + \"\\n\")\n",
    "    if sentence_buf:\n",
    "        f_sent.writelines(sentence_buf)\n",
    "    if recombined_buf:\n",
    "        f_recombined.writelines(recombined_buf)\n",
    "    if word_buf:\n",
    "        f_word.writelines(word_buf)\n",
    "\n",
    "    print(f\"‚úÖ Processed Batch {batch_num} - {len(batch_lines)} lines\")\n",
    "\n",
    "# Run\n",
    "# process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1aa7702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# input_file = \"gu_meta_part_1.txt\"\n",
    "# sentence_file = \"gu_meta_part_1.txt_sentences.txt\"\n",
    "# word_file = \"gu_meta_part_1.txt_words.txt\"\n",
    "# recombined_file = \"gu_meta_part_1.txt_recombined.txt\"\n",
    "# email_file = \"gu_meta_part_1.txt_emails.txt\"\n",
    "# url_file=\"gu_meta_part_1.txt_url.txt\"\n",
    "\n",
    "# process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a80e1917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed Batch 1 - 10000 lines\n",
      "‚úÖ Processed Batch 2 - 10000 lines\n",
      "‚úÖ Processed Batch 3 - 10000 lines\n",
      "‚úÖ Processed Batch 4 - 10000 lines\n",
      "‚úÖ Processed Batch 5 - 10000 lines\n",
      "‚úÖ Processed Batch 6 - 10000 lines\n",
      "‚úÖ Processed Batch 7 - 4992 lines\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "input_file = \"gu_meta_part_2.txt\"\n",
    "sentence_file = \"gu_meta_part_2sentences.txt\"\n",
    "word_file = \"gu_meta_part_2_words.txt\"\n",
    "recombined_file = \"gu_meta_part_2_recombined.txt\"\n",
    "email_file = \"gu_meta_part_2_emails.txt\"\n",
    "url_file=\"gu_meta_part_2_url.txt\"\n",
    "\n",
    "process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaf4d39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed Batch 1 - 10000 lines\n",
      "‚úÖ Processed Batch 2 - 10000 lines\n",
      "‚úÖ Processed Batch 3 - 10000 lines\n",
      "‚úÖ Processed Batch 4 - 10000 lines\n",
      "‚úÖ Processed Batch 5 - 10000 lines\n",
      "‚úÖ Processed Batch 6 - 10000 lines\n",
      "‚úÖ Processed Batch 7 - 2890 lines\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "input_file = \"gu_meta_part_3.txt\"\n",
    "sentence_file = \"gu_meta_part_3_sentences.txt\"\n",
    "word_file = \"gu_meta_part_3_words.txt\"\n",
    "recombined_file = \"gu_meta_part_3_recombined.txt\"\n",
    "email_file = \"gu_meta_part_3_emails.txt\"\n",
    "url_file=\"gu_meta_part_3_url.txt\"\n",
    "\n",
    "process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "449465eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed Batch 1 - 10000 lines\n",
      "‚úÖ Processed Batch 2 - 10000 lines\n",
      "‚úÖ Processed Batch 3 - 1109 lines\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "input_file = \"gu_meta_part_4.txt\"\n",
    "sentence_file = \"gu_meta_part_4_sentences.txt\"\n",
    "word_file = \"gu_meta_part_4_words.txt\"\n",
    "recombined_file = \"gu_meta_part_4_recombined.txt\"\n",
    "email_file = \"gu_meta_part_4_emails.txt\"\n",
    "url_file=\"gu_meta_part_4_url.txt\"\n",
    "\n",
    "process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f88d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Corpus Statistics Calculation\n",
    "\n",
    "# # Read tokenized files\n",
    "# with open(\"sentences.txt\", 'r', encoding='utf-8') as f:\n",
    "#     tokenized_sentences = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# with open(\"words.txt\", 'r', encoding='utf-8') as f:\n",
    "#     tokenized_words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# with open(\"emails.txt\", 'r', encoding='utf-8') as f:\n",
    "#     email = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# with open(\"url.txt\", 'r', encoding='utf-8') as f:\n",
    "#     url = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# # i. Total number of sentences\n",
    "# total_sentences = len(tokenized_sentences)\n",
    "\n",
    "# # ii. Total number of words\n",
    "# total_words = len(tokenized_words)\n",
    "\n",
    "# # iii. Total number of characters (excluding whitespace)\n",
    "# total_characters = sum(len(token) for token in tokenized_words)\n",
    "\n",
    "# # iv. Average Sentence Length\n",
    "# avg_sentence_length = total_words / total_sentences if total_sentences else 0\n",
    "\n",
    "# # v. Average Word Length\n",
    "# avg_word_length = total_characters / total_words if total_words else 0\n",
    "\n",
    "# # vi. Type/Token Ratio\n",
    "# unique_tokens = len(set(tokenized_words))\n",
    "# ttr = unique_tokens / total_words if total_words else 0\n",
    "\n",
    "# # üìä Print Results\n",
    "# print(\" Statistics\")\n",
    "# print(f\"Total Sentences           : {total_sentences}\")\n",
    "# print(f\"Total Words               : {total_words}\")\n",
    "# print(f\"Total Emails              : {len(email)}\")\n",
    "# print(f\"Total URLs                : {len(url)}\")\n",
    "# print(f\"Total Characters          : {total_characters}\")\n",
    "# print(f\"Average Sentence Length   : {avg_sentence_length:.2f} words/sentence\")\n",
    "# print(f\"Average Word Length       : {avg_word_length:.2f} chars/word\")\n",
    "# print(f\"Type/Token Ratio (TTR)    : {ttr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7668872d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m count\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# --- Sentences ---\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m total_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mcount_non_empty_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentences.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# --- Words ---\u001b[39;00m\n\u001b[0;32m     43\u001b[0m total_words, total_chars, unique_tokens \u001b[38;5;241m=\u001b[39m process_file_stats(\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, count_chars\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collect_unique\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     45\u001b[0m )\n",
      "Cell \u001b[1;32mIn[1], line 33\u001b[0m, in \u001b[0;36mcount_non_empty_lines\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     31\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BATCH_SIZE = 50000  # Adjust based on memory and speed needs\n",
    "\n",
    "def process_file_stats(file_path, count_chars=False, collect_unique=False):\n",
    "    \"\"\"Process a tokenized file in batches for stats.\"\"\"\n",
    "    total_lines = 0\n",
    "    total_chars = 0\n",
    "    unique_tokens = set() if collect_unique else None\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        batch = []\n",
    "        for line in f:\n",
    "            token = line.strip()\n",
    "            if token:\n",
    "                total_lines += 1\n",
    "                if count_chars:\n",
    "                    total_chars += len(token)\n",
    "                if collect_unique:\n",
    "                    unique_tokens.add(token)\n",
    "\n",
    "            # Optional batch handling (if you need per-batch ops)\n",
    "            if len(batch) >= BATCH_SIZE:\n",
    "                batch.clear()\n",
    "\n",
    "    return total_lines, total_chars, unique_tokens\n",
    "\n",
    "\n",
    "def count_non_empty_lines(file_path):\n",
    "    \"\"\"Count non-empty lines quickly.\"\"\"\n",
    "    count = 0\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "# --- Sentences ---\n",
    "total_sentences = count_non_empty_lines(\"sentences.txt\")\n",
    "\n",
    "# --- Words ---\n",
    "total_words, total_chars, unique_tokens = process_file_stats(\n",
    "    \"words.txt\", count_chars=True, collect_unique=True\n",
    ")\n",
    "\n",
    "# --- Emails & URLs ---\n",
    "total_emails = count_non_empty_lines(\"emails.txt\")\n",
    "total_urls = count_non_empty_lines(\"url.txt\")\n",
    "\n",
    "# --- Stats ---\n",
    "avg_sentence_len = total_words / total_sentences if total_sentences else 0\n",
    "avg_word_len = total_chars / total_words if total_words else 0\n",
    "ttr = len(unique_tokens) / total_words if total_words else 0\n",
    "\n",
    "# --- Print ---\n",
    "print(\"üìä Corpus Statistics (Batch Mode)\")\n",
    "print(f\"Total Sentences         : {total_sentences}\")\n",
    "print(f\"Total Words             : {total_words}\")\n",
    "print(f\"Total Emails            : {total_emails}\")\n",
    "print(f\"Total URLs              : {total_urls}\")\n",
    "print(f\"Total Characters        : {total_chars}\")\n",
    "print(f\"Average Sentence Length : {avg_sentence_len:.2f} words/sentence\")\n",
    "print(f\"Average Word Length     : {avg_word_len:.2f} chars/word\")\n",
    "print(f\"Type/Token Ratio (TTR)  : {ttr:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
